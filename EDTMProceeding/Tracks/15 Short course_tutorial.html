<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>15 Short course/tutorial</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
        h1 { color: #2c3e50; border-bottom: 2px solid #3498db; }
        .paper { margin: 20px 0; padding: 15px; border-left: 4px solid #3498db; }
        .title { font-weight: bold; color: #2c3e50; }
        .authors { color: #7f8c8d; font-size: 0.9em;font-style: italic; }
        .abstract { color: #666; margin-top: 15px; font-size: 0.95em; }
        .invited { color: #27ae60; font-weight: bold; }
    </style>
</head>
<body>
    <h1>15 Short course/tutorial </h1>
    
        <div class="paper">
            <h3 class="title">Optimal transfer learning strategies for property predictions in materials science<span class="invited">[Invited]</span></h3>
            <div class="authors">Sai Gautam Gopalakrishnan,Reshma Devi,Keith Butler</div>
            <div class="abstract">Materials science is a domain characterised by ‘small’ datasets (i.e., < 10,000 datapoints) of critical properties that govern performance of various applications and devices. For instance, there are no large, reliable datasets available for several key ‘performance determining’ metrics in energy applications, such as diffusivities in battery electrodes, carrier recombination rates in photovoltaics, and molecular adsorption energies for catalysis. On the other hand, there are reasonably ‘large' datasets (> 100,000 datapoints) available on some properties, such as, bulk formation enthalpies, computed band structures, and crystal structures across wide chemical spaces. Thus, if key chemical, compositional, and structural trends can be captured in available large datasets and subsequently transferred (or re-learnt), it will enable the use of deep learning and graph based neural network models in smaller datasets as well. Hence, my talk will explore the utility of current transfer learning (TL) approaches that are available for computational materials science and identify optimal ways to employ TL-based strategies. Specifically, TL involves training a neural network model on a larger dataset and subsequently retraining a fraction of the model on a smaller dataset. I will quantify the accuracy, transferability, and efficiency of TL models compared to models that have been trained from scratch. Finally, I will focus on TL models that can generalise over multi-properties during pre-training and can efficiently be re-trained on small datasets, which pave the way towards creating more general, foundational models, in the near future.</div>
        </div>
        
        <div class="paper">
            <h3 class="title">A Digital Twin for Advanced Manufacturing of Materials<span class="invited">[Invited]</span></h3>
            <div class="authors">Dipayan Sanpui,Anirban Chandra,Henry Chan,Sukriti Manna,Subramanian Sankaranarayanan</div>
            <div class="abstract">There is a clear need in advanced manufacturing to develop Digital Twins that combine AI/ML with physics-based multiscale models to understand structure-property-processing relationships in materials. Additive manufacturing (AM) is one such transformative technique that allows creation of components with complex geometries layer-by-layer that are prohibitively difficult to achieve with traditional manufacturing techniques. Existing commercial and open-source software allow users to carry out materials simulations but lack predictive power to model manufacturing processes with complex thermal history and do not allow users the flexibility to design microstructures and hence functionality tailored to suit their specific needs. We aim to overcome this critical barrier by developing a high-performance user-friendly Digital Twin that will enable end users to exhaustively explore, identify and design time-dependent AM protocols that achieve tailor-made microstructures. A subset of the processing conditions that lead to the most promising microstructures and functionality will then be experimentally manufactured and validated. Our Digital Twin employs a kinetic Monte Carlo (KMC) based model of the AM process to simulate microstructural evolution for a diverse set of experimentally relevant processing conditions and uses AI/ML to explore the relationship between microstructural features and processing conditions. We also present probabilistic machine learning methodologies, namely Gaussian Process Regression (GPR) and Probabilistic Bayesian Neural Networks (BNNs) algorithms for precise dimensional control and uncertainty quantification in additive manufacturing.</div>
        </div>
        
        <div class="paper">
            <h3 class="title">HIGH TEMPERATURE SUPERFLUORESCENCE EMITTING PEROVSKITE MATERIALS<span class="invited">[Invited]</span></h3>
            <div class="authors">Kenan Gundogdu,Franky So,Melike Biliroglu,Gamze Findik</div>
            <div class="abstract">Light-matter interactions can create and manipulate collective many-body phases in solids, offering significant potential for emerging quantum technologies. However, these collective quantum states are often fragile, with short decoherence and dephasing times, which restricts their stability limited number of materials under specific conditions, such as cryogenic temperatures or high external fields. Recent studies on perovskite superfluorescence prove that some material properties can intrinsically sustain collective quantum coherence. Here we will discuss some of these early results.</div>
        </div>
        
        <div class="paper">
            <h3 class="title">Knowledge discovery from microscopic image data using an explainable AI “extended free energy model”<span class="invited">[Invited]</span></h3>
            <div class="authors">Masato Kotsugi</div>
            <div class="abstract">This study places a spotlight on the role of explainable AI, embodied in the extended Landau free-energy model [1-5], as a transformative tool for understanding and designing magnetic systems. By integrating physics-based features with data-driven methodologies, this model facilitates transparent, interpretable, and actionable insights into magnetization reversal processes, addressing longstanding challenges in mesoscale physics.
Magnetization reversal in nanomagnets is a cornerstone phenomenon for advancing spintronic devices, demanding novel analytical approaches to decode its complexity. Leveraging the capabilities of the extended Landau free-energy model alongside topological data analysis (TDA), this research bridges the gap between microstructures and macroscopic properties. Persistent homology (PH), ridge regression (RR), and principal component analysis (PCA) are employed to quantify magnetic domain complexities, identify energy contributions, and visualize magnetization dynamics. This integrated framework not only reveals the causal relationships driving energy barriers and domain transitions but also offers a pathway for energy-efficient and highly optimized magnetic device design.
</div>
        </div>
        
    <p><a href="index.html">← Back</a></p>
</body>
</html>